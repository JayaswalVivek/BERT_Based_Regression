<h3> DistilBERT for a Language-based Regression Task </h3>
<b> Data Source: Kaggle </b> </br>
<b> URL: https://www.kaggle.com/competitions/commonlitreadabilityprize</b> </br> </br>

An implementation of DistilBERT for Kaggle's "CommonLit Readability Prize" challenge. The source data sets can be downloaded from Kaggle's website.

**RMSE (as per Kaggle dashboard): 0.606**
